{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c503aa0-aaef-4624-93eb-fa31be74a44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import typing\n",
    "from numpy.random import default_rng\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "36f91d48-f03e-4b36-9f5a-cb416df34060",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Engine():\n",
    "    \n",
    "    #initialize dataset, a random trajectory, and current cycle\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.service = True\n",
    "        self.episode = self.get_trajectory(np.random.randint(low=1, high=79, size=1))\n",
    "        self.cycle = 0\n",
    "        \n",
    "    #get random trajectory\n",
    "    def get_trajectory(self, engine_id):\n",
    "        return self.dataset[self.dataset.engine_id==engine_id.item()].health_indicator.to_numpy()\n",
    "    \n",
    "    #return current state\n",
    "    def get_state(self):\n",
    "        return self.cycle, self.episode[self.cycle]\n",
    "    \n",
    "    #take action\n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            #failure occurs\n",
    "            if self.cycle+1 == self.episode.size:\n",
    "                res = (None, -78)\n",
    "                self.service = False\n",
    "            #continued operation, return 1 and continue episode\n",
    "            else:\n",
    "                res = (self.episode[self.cycle+1], self.episode[self.cycle])\n",
    "                self.cycle+=1\n",
    "        elif action == 1:\n",
    "            self.cycle=0+int(np.random.uniform(0,50))\n",
    "            res = (self.episode[self.cycle], -self.episode[self.cycle])\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "f0df7727-1a04-4518-ad2c-a4d86df7e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#environment class\n",
    "class Environment():\n",
    "    \n",
    "    #initialize dataset, a random trajectory, and current cycle\n",
    "    def __init__(self, fleet_size=5):\n",
    "        self.dataset = pd.read_csv('train.csv')\n",
    "        self.fleet_size = fleet_size\n",
    "        self.fleet = []\n",
    "        for engine in range(fleet_size):\n",
    "            self.fleet.append(Engine(self.dataset))\n",
    "        self.balance = 100\n",
    "        self.repair_cost = 25\n",
    "        \n",
    "    def get_state(self):\n",
    "        healths = []\n",
    "        cycles = []\n",
    "        for engine in self.fleet:\n",
    "            if engine.service:\n",
    "                cycle, health = engine.get_state()\n",
    "                healths.append(health)\n",
    "                cycles.append(cycle)\n",
    "            else:\n",
    "                healths.append(-1)\n",
    "                cycles.append(-1)\n",
    "        return np.array([self.balance]+cycles+healths).flatten()\n",
    "        \n",
    "    def take_action(self, actions):\n",
    "        #initialize reward as zero\n",
    "        reward = 0\n",
    "        #iterate over action-engine pair\n",
    "        for action, engine in zip(actions, self.fleet):\n",
    "            #hold\n",
    "            if action == 0:\n",
    "                if engine.service:\n",
    "                    _, r = engine.step(action)\n",
    "                    reward+=r\n",
    "            #replace\n",
    "            elif action == 1:\n",
    "                #perform replacement if enough money\n",
    "                if self.balance >= self.repair_cost:\n",
    "                    self.balance-=self.repair_cost\n",
    "                    if engine.service:\n",
    "                        _, r = engine.step(action)\n",
    "                        reward+=r\n",
    "                #penalize if not enough money\n",
    "                else:\n",
    "                    reward-=10\n",
    "                    \n",
    "        #return reward and terminal flag\n",
    "        if np.all([engine.service==False for engine in self.fleet]) or self.balance<=0:\n",
    "            return reward, True\n",
    "        else:\n",
    "            return reward, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "a6d03d67-cb90-4729-8788-f15f7527559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transition class for replay memory\n",
    "class Transition():\n",
    "    \n",
    "    def __init__(self, state, action, state_new, reward, term ):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.state_new = state_new\n",
    "        self.reward = reward\n",
    "        self.term = term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "408527ac-f4cb-4e7b-b6cd-59a22614d16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prioritized replay memory class\n",
    "class PrioritizedReplayMemory:\n",
    "    \"\"\"Fixed-size buffer to store priority, Experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 batch_size: int,\n",
    "                 buffer_size: int,\n",
    "                 alpha: float = 0.0,\n",
    "                 random_state: np.random.RandomState = None) -> None:\n",
    "        \"\"\"\n",
    "        Initialize an ExperienceReplayBuffer object.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        buffer_size (int): maximum size of buffer\n",
    "        batch_size (int): size of each training batch\n",
    "        alpha (float): Strength of prioritized sampling. Default to 0.0 (i.e., uniform sampling).\n",
    "        random_state (np.random.RandomState): random number generator.\n",
    "        \n",
    "        \"\"\"\n",
    "        self._batch_size = batch_size\n",
    "        self._buffer_size = buffer_size\n",
    "        self._buffer_length = 0 # current number of prioritized experience tuples in buffer\n",
    "        self._buffer = np.empty(self._buffer_size, dtype=[(\"priority\", np.float32), (\"transition\", Transition)])\n",
    "        self._alpha = alpha\n",
    "        self._random_state = np.random.RandomState() if random_state is None else random_state\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Current number of prioritized experience tuple stored in buffer.\"\"\"\n",
    "        return self._buffer_length\n",
    "\n",
    "    @property\n",
    "    def alpha(self):\n",
    "        \"\"\"Strength of prioritized sampling.\"\"\"\n",
    "        return self._alpha\n",
    "\n",
    "    @property\n",
    "    def batch_size(self) -> int:\n",
    "        \"\"\"Number of experience samples per training batch.\"\"\"\n",
    "        return self._batch_size\n",
    "    \n",
    "    @property\n",
    "    def buffer_size(self) -> int:\n",
    "        \"\"\"Maximum number of prioritized experience tuples stored in buffer.\"\"\"\n",
    "        return self._buffer_size\n",
    "\n",
    "    def add(self, transition: Transition) -> None:\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        priority = 1.0 if self.is_empty() else self._buffer[\"priority\"].max()\n",
    "        if self.is_full():\n",
    "            if priority > self._buffer[\"priority\"].min():\n",
    "                idx = self._buffer[\"priority\"].argmin()\n",
    "                self._buffer[idx] = (priority, transition)\n",
    "            else:\n",
    "                pass # low priority experiences should not be included in buffer\n",
    "        else:\n",
    "            self._buffer[self._buffer_length] = (priority, transition)\n",
    "            self._buffer_length += 1\n",
    "\n",
    "    def is_empty(self) -> bool:\n",
    "        \"\"\"True if the buffer is empty; False otherwise.\"\"\"\n",
    "        return self._buffer_length == 0\n",
    "    \n",
    "    def is_full(self) -> bool:\n",
    "        \"\"\"True if the buffer is full; False otherwise.\"\"\"\n",
    "        return self._buffer_length == self._buffer_size\n",
    "    \n",
    "    def sample(self, beta: float) -> typing.Tuple[np.array, np.array, np.array]:\n",
    "        \"\"\"Sample a batch of experiences from memory.\"\"\"\n",
    "        # use sampling scheme to determine which experiences to use for learning\n",
    "        ps = self._buffer[:self._buffer_length][\"priority\"]\n",
    "        sampling_probs = ps**self._alpha / np.sum(ps**self._alpha)\n",
    "        idxs = self._random_state.choice(np.arange(ps.size),\n",
    "                                         size=self._batch_size,\n",
    "                                         replace=True,\n",
    "                                         p=sampling_probs)\n",
    "        \n",
    "        # select the experiences and compute sampling weights\n",
    "        transitions = self._buffer[\"transition\"][idxs]        \n",
    "        weights = (self._buffer_length * sampling_probs[idxs])**-beta\n",
    "        normalized_weights = weights / weights.max()\n",
    "        \n",
    "        return idxs, transitions, normalized_weights\n",
    "\n",
    "    def update_priorities(self, idxs: np.array, priorities: np.array) -> None:\n",
    "        \"\"\"Update the priorities associated with particular experiences.\"\"\"\n",
    "        self._buffer[\"priority\"][idxs] = priorities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "490edb40-778c-4ef3-8b25-b8735ecc0dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dqn model class\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self, input_dim, output_dim).__init__()\n",
    "        self.lin1 = nn.Linear(input_dim,32)\n",
    "        self.lin2 = nn.Linear(32,64)\n",
    "        self.lin3 = nn.Linear(64,32)\n",
    "        self.lin4 = nn.Linear(32,output_dim)\n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.lin3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = F.relu(self.lin4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "85d3375c-75bb-4e40-aa86-104abd2fa0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get action given net, state, and probability of random action\n",
    "def get_action(net, state, epsilon, size):\n",
    "    with torch.no_grad():\n",
    "        greedy = np.random.choice([True, False], p=[1-epsilon, epsilon])\n",
    "        if greedy:\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "            q_values = net(state)\n",
    "            q_values = q_values.reshape(-1,2)\n",
    "            actions = torch.argmax(q_values, dim=1)\n",
    "        else:\n",
    "            actions = np.random.choice([1,0], size=fleet_size, p=[epsilon, 1-epsilon])\n",
    "        return action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-rl",
   "language": "python",
   "name": "env-rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
